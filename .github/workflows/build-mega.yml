name: Build US+Canada Mega-Rarities Map

on:
  schedule:
    - cron: "13 9 * * *"   # Daily 09:13 UTC
  workflow_dispatch:
    inputs:
      skip_commit:
        description: "Do not write to the repo. Builds files only in the runner."
        type: boolean
        default: false
      mode:
        description: "aba5_only for strict ABA Code 5. union also includes national scarcities."
        type: choice
        options: [aba5_only, union]
        default: aba5_only
      back_days_recent:
        description: "Days of recent 'notable' records to fetch for US and CA."
        type: string
        default: "1"
      per_species_max:
        description: "Max markers per species to keep the page fast."
        type: string
        default: "2"

permissions:
  contents: write

defaults:
  run:
    shell: bash

jobs:
  mega:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euxo pipefail
          python -m pip install --upgrade pip
          pip install pandas requests folium
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Ensure inputs exist
        run: |
          set -euxo pipefail
          test -f scripts/data/ABA_Checklist.csv
          test -f scripts/data/eBird_taxonomy_v2024.csv
          mkdir -p docs/mega

      # 1) Extract ABA Code 5 rows from the checklist - tolerant of headers and 5*, 5?
      - name: Extract ABA Code 5 from ABA checklist
        run: |
          python - <<'PY'
          import csv, json, pathlib, sys, re, io

          aba_path = pathlib.Path("scripts/data/ABA_Checklist.csv")
          raw_text = aba_path.read_text(encoding="utf-8", errors="replace")

          lines = raw_text.splitlines()
          header_idx = None
          sep = ","

          def looks_like_header(line, sep_char):
            toks = [t.strip().strip('"').lower() for t in line.split(sep_char)]
            if len(toks) < 3:
              return False
            has_code = any("code" in t for t in toks)
            has_name = any("english" in t for t in toks) or any("scientific" in t for t in toks) or any("common" in t for t in toks)
            return has_code and has_name

          for i, line in enumerate(lines):
            if line.count(",") >= 2 and looks_like_header(line, ","):
              header_idx = i; sep = ","; break
          if header_idx is None:
            for i, line in enumerate(lines):
              if line.count("\t") >= 2 and looks_like_header(line, "\t"):
                header_idx = i; sep = "\t"; break

          if header_idx is None:
            print("Could not find a CSV or TSV header. First 3 lines:")
            print("\n".join(lines[:3]))
            sys.exit("Could not locate ABA Code and Name columns in ABA_Checklist.csv")

          trimmed = "\n".join(lines[header_idx:])

          class FlexibleReader(csv.DictReader):
            def __init__(self, s, delimiter):
              super().__init__(io.StringIO(s), delimiter=delimiter)

          reader = FlexibleReader(trimmed, delimiter=sep)

          CODE_KEYS = {"aba code","code","aba_code","rarity code","aba rarity code","aba checklist code"}
          ENG_KEYS  = {"english name","primary_com_name","common name","name","primary com name"}
          SCI_KEYS  = {"scientific name","sci_name","scientific"}

          norm_header = {h.strip().lower(): h for h in reader.fieldnames}

          def pick(candidates):
            for k in candidates:
              if k in norm_header:
                return norm_header[k]
            return None

          code_k = pick(CODE_KEYS)
          eng_k  = pick(ENG_KEYS)
          sci_k  = pick(SCI_KEYS)

          if not code_k or not (eng_k or sci_k):
            print("Detected header:", reader.fieldnames)
            sys.exit("Could not locate ABA Code and Name columns after trimming preamble")

          rows = []
          for r in reader:
            code_val = str(r.get(code_k, "")).strip()
            if re.match(r"^\s*5\b", code_val):  # accept 5, 5*, 5?, 5.0, etc.
              rows.append({
                "english": (r.get(eng_k, "") or "").strip(),
                "sci": (r.get(sci_k, "") or "").strip()
              })

          out_dir = pathlib.Path("docs/mega"); out_dir.mkdir(parents=True, exist_ok=True)
          (out_dir / "aba5_raw.json").write_text(json.dumps(rows, indent=2), encoding="utf-8")

          label = "TAB" if sep == "\t" else "COMMA"
          print(f"[info] Header found on line {header_idx+1} using delimiter {label}")
          print(f"[info] Extracted {len(rows)} ABA Code 5 rows from {aba_path.name}")
          PY

      # 2) Resolve those rows to eBird SPECIES_CODE using the taxonomy file
      - name: Resolve ABA 5 to eBird species codes
        run: |
          python - <<'PY'
          import csv, json, pathlib, re, unicodedata

          tax_path = pathlib.Path("scripts/data/eBird_taxonomy_v2024.csv")
          raw = json.loads(pathlib.Path("docs/mega/aba5_raw.json").read_text(encoding="utf-8"))

          def norm(s):
            s = (s or "").strip().lower()
            s = unicodedata.normalize("NFKD", s)
            s = s.replace("Ã—", "x")
            s = re.sub(r"\s+", " ", s)
            return s

          by_sci, by_eng = {}, {}
          with tax_path.open(encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
              sci = norm(row.get("SCI_NAME"))
              eng = norm(row.get("PRIMARY_COM_NAME"))
              code = (row.get("SPECIES_CODE") or "").strip()
              if code:
                if sci: by_sci[sci] = code
                if eng: by_eng[eng] = code

          matched, unresolved = [], []
          for rec in raw:
            sci = norm(rec.get("sci"))
            eng = norm(rec.get("english"))
            code = by_sci.get(sci) or by_eng.get(eng)
            if code: matched.append(code)
            else: unresolved.append(rec)

          matched = sorted(set(c for c in matched if c))
          out_dir = pathlib.Path("docs/mega"); out_dir.mkdir(parents=True, exist_ok=True)
          (out_dir / "aba5.json").write_text(json.dumps(matched, indent=2), encoding="utf-8")
          (out_dir / "aba5_unresolved.json").write_text(json.dumps(unresolved, indent=2), encoding="utf-8")

          print(f"[info] ABA 5 rows: {len(raw)}  -> matched codes: {len(matched)}  unresolved: {len(unresolved)}")
          print("[info] First few codes:", json.dumps(matched[:10]))
          PY
        # This mirrors your existing resolver logic  

      - name: Sanity checks for aba5.json
        run: |
          set -euxo pipefail
          test -s docs/mega/aba5.json
          echo "size:" $(jq 'length' docs/mega/aba5.json)
          # fail if zero
          test "$(jq 'length' docs/mega/aba5.json)" -gt 0

      # 3) Build the mega map using your scripts/build_mega_map.py
      - name: Build mega map
        env:
          EBIRD_API_KEY: ${{ secrets.EBIRD_API_KEY }}
          MEGA_MODE: ${{ inputs.mode }}
          MEGA_BACK_DAYS_RECENT: ${{ inputs.back_days_recent }}
          MEGA_BACK_DAYS_SCARCITY: "365"
          MEGA_NATIONAL_MAX: "25"
          MEGA_PER_SPECIES_MAX: ${{ inputs.per_species_max }}
        run: |
          set -euxo pipefail
          python scripts/build_mega_map.py
          echo "----- docs/mega listing -----"
          ls -lh docs/mega || true

      - name: Upload mega diagnostics
        uses: actions/upload-artifact@v4
        with:
          name: mega-diagnostics
          path: |
            docs/mega/aba5.json
            docs/mega/aba5_unresolved.json
            docs/mega/index.html
            docs/mega/*.json
            docs/mega/*.csv

      - name: Commit and push docs/mega
        if: ${{ inputs.skip_commit != true }}
        run: |
          set -euxo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/mega
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Build US+Canada mega-rarities map (${GITHUB_RUN_ID})"
            git pull --rebase || true
            git push
